<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">3D Vision-Language Model for Generalized Robotic Manipulation</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Spring 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Pickachu</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="./files/media/pranay_junare.jpeg">
            
            
          </div>
          <p>
                        
              Pranay Junare
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/media/aditya_bidwai.jpg">
            
          </div>
          <p>
            
            Aditya Bidwai
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="files/media/Samra Huseynova.jpg">            
            
          </div>
          <p>
              Samra Huseynova
          </p>
        </div>
        
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="files/media/FINAL_POSTER_00_Pickachu_32-40-NLP_v2.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Poster</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Generalized language-conditioned robotic manipulation remains a key challenge in the integration of natural language in robotics or embodied AI. While previous approaches have leveraged vision-language models (VLMs) like Contrastive Language-Image Pretraining (CLIP), they often struggle with generalization across diverse tasks and environments. In this project, we focus on improving language-conditioned robotic manipulation across varied 3D tasks, object types, and environments. We use Sigmoidal Language-Image Pre-Training 2 (SigLIP2), a more recent VLM, as our vision-language backbone and condition a transformer-based policy network to predict robot actions. The model is trained in an end-to-end fashion using Imitation Learning. The main objective of our project is to improve generalization across varied 3D tasks, object types, and environments, advancing the robustness of VLM-guided robotic manipulation. We want to understand whether multi-modal learning can help in performing complex 3D manipulation tasks.</p>

<hr>

<h2 id="teaser">Block Diagram</h2>

<!-- <p>A figure that conveys the main idea behind the project or the main application being addressed. This figure is from <a href="https://arxiv.org/pdf/2210.07469">StyLEx</a>.</p>
 -->
<p class="sys-img"><img src="./files/teaser_figure.jpeg" alt="imgname"></p>


<hr>

<!-- <h2 id="introduction">Introduction / Background / Motivation</h2> -->
<h2 id="introduction">Introduction</h2>


<p>
  We aim to develop a general-purpose robotic manipulation system that understands natural language commands and performs diverse 3D object tasks using a dual-arm setup. Our objective is to explore whether vision-language models (VLMs) can help robots perform a wide range of manipulation tasks in varied settings.
</p>

<p>
  Most current approaches rely on task-specific models or single-arm robots, which limits their ability to generalize across different environments, objects, or instructions. While recent vision-language models have shown promise, they are mostly tested on simple tasks and do not support bimanual coordination or more complex real-world execution.
</p>

<p>
Potentially, this work can significantly improve the scalability and usefulness of real-world robot assistants - particularly in environments like homes, warehouses, or labs, where tasks vary in complexity and structure. We believe that building robust, language-grounded, multi-task policies will be essential for real-world deployment of general-purpose robots.
</p>

<!-- </p>
<p>
Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.
</p> -->

<hr>

<h2 id="approach">Approach</h2>

<p>
Our goal is to learn a single transformer-based model for a variety of different language-conditioned robot manipulation tasks. The input consists of (1) a language description of the task, (2) the current visual state (from RGB-D camera(s)), and (3) the current gripper state (open or closed).  The model should predict an action, specified by a target end-effector pose and gripper state at the next key-frame. The key-frames represent important or bottleneck steps of the end-effector during the task execution such as a pre-pick, grasp, or place pose. Our dataset consists of pre-generated RLBench (simulations) demonstrations open-sourced by (Shridhar et al., 2022). The authors have divided this dataset into train, validate, and test, containing 100, 25, and 25 episodes, respectively. This dataset is used to train the model with Imitation learning approach of behavior cloning. Our Transformer architecture is visualized in the image below. The input to the transformer is a language description of the task and virtual images of the scene point cloud. The text is converted into token embeddings using the pretrained CLIP model, while the virtual images are converted into token embeddings via patchify operations. For each virtual image, tokens belonging to the same image are processed via four attention layers. Finally, the processed image tokens as well as the language tokens are jointly processed using four attention layers. The 3D action is inferred using the resulting image tokens.
</p>

<p class="sys-img"><img src="./files/media/transformer_arch.png" alt="imgname"></p>


<hr>
    
<h2 id="results">Results</h2>
<p>
We trained Transformer model on a single task—“put groceries into cupboard”—using a total of 100 task variations over 15 epochs. The training setup used a batch size of 4, three data loading workers, and a task-uniform sampling strategy. The process took around 16 hours to complete. We used the LAMB optimizer with a learning rate of 1.25e-5 and applied translation augmentation of ±12.5 cm and rotation augmentation of ±45° around the z-axis. Mixed precision training (AMP) and memory-efficient 8-bit optimizers were enabled to help with performance and GPU memory usage.  The Transformer architecture follows a two-stage design, where the model first predicts a coarse location using multiple fixed RGB-D cameras, and then refines the prediction using a zoomed-in view. The simulation used four RGB-D cameras in total: a front view, top view, right-side view, and a gripper-mounted camera. The gripper camera provides a close-up view from the robot’s perspective, which is especially useful for fine-grained manipulation. Loss curves (e.g., total loss, grip loss, and rotation loss) consistently decreased throughout training, indicating good convergence. Evaluation was done on 25 new variations of the same task using CoppeliaSim, which also allowed us to visualize results. The model was able to follow language instructions reasonably well, differentiate between objects, and pick items based on colors—even when the exact color was not in view, it would choose the closest match. However, we also found some limitations: the model struggles with understanding shape and texture, has trouble identifying orientation of the shelves, and sometimes fails to grasp or place objects correctly. It’s also sensitive to misspellings in the language commands. While it can differentiate between different objects and colors to some extent, it still needs improvement in dealing with more complex scenes and instructions. Overall, the model showed solid performance on the chosen task, but further work is needed to make it more robust and accurate in varied conditions.
</p>
<br>

<hr>
<h2 id="images">Training Loss Curves</h2>

<div class="image-container" style="display: flex; justify-content: space-between; gap: 20px; flex-wrap: wrap;">
  <!-- Image 1 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_rot_loss_x.png" alt="Demo Image 1" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 1: Rotation loss - X</p>
  </div>

  <!-- Image 2 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_rot_loss_y.png" alt="Demo Image 2" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 2: Rotation loss - Y</p>
  </div>

  <!-- Image 3 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_rot_loss_z.png" alt="Demo Image 3" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 3: Rotation loss - Z</p>
  </div>
</div>  

<div class="image-container" style="display: flex; justify-content: space-between; gap: 20px; flex-wrap: wrap;">
  <!-- Image 1 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_grip_loss.png" alt="Demo Image 1" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 4: Gripper loss</p>
  </div>

  <!-- Image 2 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_collision_loss.png" alt="Demo Image 2" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 5: Collision loss</p>
  </div>

  <!-- Image 3 -->
  <div class="image-item" style="text-align: center; flex: 1;">
    <img src="./files/media/train_total_loss.png" alt="Demo Image 3" style="width: 100%; max-width: 300px; height: auto;">
    <p style="margin-top: 8px; font-weight: bold;">Figure 6: Total loss</p>
  </div>
</div>



<br><br>


<hr>
<h2 id="videos">Demo Videos</h2>

<div class="video-container" style="display: flex; justify-content: space-between; gap: 20px; flex-wrap: wrap;">
  <!-- First Video + Caption -->
  <div class="video-item" style="text-align: center;">
    <video width="450" height="200" controls>
      <source src="./files/media/vid_place_crackers_in_cupboard.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <p style="margin-top: 8px; font-weight: bold;">Task 1.1: Place Cracker box in the Cupboard</p>
  </div>

  <!-- Second Video + Caption -->
  <div class="video-item" style="text-align: center;">
    <video width="450" height="200" controls>
      <source src="./files/media/vid_place_sugar_in_cupboard.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <p style="margin-top: 8px; font-weight: bold;">Task 1.2: Place Sugar box in the cupboard</p>
  </div>
</div>
<hr>




<h2 id="conclusion">Conclusion and Future Work</h2>
<p>

 In conclusion, our training and evaluation of the baseline model on the “put groceries into cupboard” task showed promising results, with the model demonstrating a reasonable understanding of language commands and visual features in simulation. While it successfully differentiated between items and handled basic manipulation, challenges remain in terms of handling shape, texture, language ambiguity, and spatial understanding. To address these limitations and move toward real-world deployment, our future work involves collecting expert demonstrations on physical hardware using a bimanual robot setup. We aim to fine-tune the current model on real-world data collected across various RLBench tasks. Preparations are already underway, including learning the SPARK system for data collection, setting up camera calibration, and preparing our hardware for high-quality demonstration recording. This transition from simulation to real-world will allow us to test the generalization ability of the model, improve its robustness, and better align it with practical applications.</p>


<hr>



<h3 id="the-timeline-and-the-highlights">Any subsection</h3>

<p>If you need to explain more about your figure</p>

  </div>
  


</body></html>
